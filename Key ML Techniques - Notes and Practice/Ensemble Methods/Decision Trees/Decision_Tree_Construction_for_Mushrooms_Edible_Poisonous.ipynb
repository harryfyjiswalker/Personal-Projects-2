{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mt0LnZxDUt-T"
      },
      "outputs": [],
      "source": [
        "#\"Advanced Learning Algorithms\" Final Task\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We have a list of mushrooms. Features are: Cap color (brown or read), stalk shape (tapering or enlarging), solitary, edible\n",
        "# Hot coding by giving brown = 1, tapering = 1, solitary = 1, edible = 1\n",
        "\n",
        "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]]) #encoding features\n",
        "y_train = np.array([1,1,0,0,1,0,0,1,1,0]) #edible or poisonous (output)"
      ],
      "metadata": {
        "id": "r5fosKh1U2l2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steps for building a decision tree:\n",
        "\n",
        "#1. Calculate the entropy at a node\n",
        "#2. Split the dataset at a node into left and right branches based on a given feature\n",
        "#3. Calculate the information gain from splitting on a given feature\n",
        "#4. Choose the feature that maximizes information gain\n",
        "#5. Use helper functions to build a decision tree by repeating splitting process until stopping criteria is met (here, max depth = 2)\n",
        "\n",
        "#Step 1. Compute entropy\n",
        "\n",
        "def compute_entropy(y):\n",
        "    \"\"\"\n",
        "    Computes the entropy for\n",
        "\n",
        "    Args:\n",
        "       y (ndarray): Numpy array indicating whether each example at a node is\n",
        "           edible (`1`) or poisonous (`0`)\n",
        "\n",
        "    Returns:\n",
        "        entropy (float): Entropy at that node\n",
        "\n",
        "    \"\"\"\n",
        "    entropy = 0.\n",
        "\n",
        "    if len(y) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    p_1 = np.sum(y == 1) / len(y) #compute fraction that are edible\n",
        "\n",
        "     # For p1 = 0 and 1, set the entropy to 0 (to handle 0log0)\n",
        "    if p_1 == 0 or p_1 == 1:\n",
        "        return 0\n",
        "    else:\n",
        "        entropy = -p_1 * np.log2(p_1) - (1 - p_1) * np.log2(1 - p_1)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "#Step 2: Split dataset\n",
        "# Splitting dataset into left ad right branches\n",
        "\n",
        "def split_dataset(X, node_indices, feature):\n",
        "\n",
        "  left_indices = []\n",
        "  right_indices = []\n",
        "\n",
        "  for i in node_indices:\n",
        "        if X[i][feature] == 1:\n",
        "            left_indices.append(i)\n",
        "        else:\n",
        "            right_indices.append(i)\n",
        "\n",
        "  return left_indices, right_indices\n",
        "\n",
        "  #Step 3: Calculate information gain\n",
        "\n",
        "  def compute_information_gain(X, y, node_indices, feature):\n",
        "\n",
        "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
        "\n",
        "    X_node, y_node = X[node_indices], y[node_indices]\n",
        "    X_left, y_left = X[left_indices], y[left_indices]\n",
        "    X_right, y_right = X[right_indices], y[right_indices]\n",
        "\n",
        "    information_gain = 0\n",
        "\n",
        "    node_entropy = compute_entropy(y_node) # entropy at node\n",
        "    left_entropy = compute_entropy(y_left) # entropy after branching\n",
        "    right_entropy = compute_entropy(y_right)\n",
        "\n",
        "    w_left = len(X_left)/len(X_node)\n",
        "    w_right = len(X_right)/len(X_node) #finding weighted fractions\n",
        "\n",
        "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
        "\n",
        "    information_gain = node_entropy - weighted_entropy\n",
        "    #info gain is change in entropy after split compared to before (want to reduce)\n",
        "\n",
        "\n",
        "    return information_gain\n",
        "\n",
        "    #Step 4: Using info gain to get best split\n",
        "\n",
        "  def get_best_split(X, y, node_indices):\n",
        "\n",
        "    max_information_gain=0\n",
        "    for feature in range(num_features):\n",
        "        information_gain = compute_information_gain(X, y, node_indices, feature)\n",
        "        if information_gain > max_information_gain:\n",
        "            max_information_gain = information_gain\n",
        "            best_feature = feature\n",
        "\n",
        "    return best_feature\n",
        "\n",
        "    #Step 4: Building the tree\n",
        "\n",
        "    # Not graded\n",
        "tree = []\n",
        "\n",
        "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
        "\n",
        "    # Maximum depth reached - stop splitting\n",
        "    if current_depth == max_depth:\n",
        "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
        "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
        "        return\n",
        "\n",
        "    # Otherwise, get best split and split the data\n",
        "    # Get the best feature and threshold at this node\n",
        "    best_feature = get_best_split(X, y, node_indices)\n",
        "\n",
        "    formatting = \"-\"*current_depth\n",
        "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
        "\n",
        "    # Split the dataset at the best feature\n",
        "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
        "    tree.append((left_indices, right_indices, best_feature))\n",
        "\n",
        "    # continue splitting the left and the right child. Increment current depth\n",
        "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
        "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)\n"
      ],
      "metadata": {
        "id": "Tb6Jqkt8Yk5q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4w62W2vZuh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}